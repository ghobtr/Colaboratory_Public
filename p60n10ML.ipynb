{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "p60n10ML.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMk4EOP00meDW4L2f7avz+r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghobtr/Colaboratory_Public/blob/main/p60n10ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7psSoHntpx7Y",
        "outputId": "fd2baa54-0ca2-4a7f-afab-3ec7cf66983e"
      },
      "source": [
        "!pip install yahoo_fin\r\n",
        "!pip install requests_html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting yahoo_fin\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/5c/6bf0c0147cc94d643e2a2413d0a9b27967e964ee99f88f26db93a0b963b8/yahoo_fin-0.8.6-py3-none-any.whl\n",
            "Installing collected packages: yahoo-fin\n",
            "Successfully installed yahoo-fin-0.8.6\n",
            "Collecting requests_html\n",
            "  Downloading https://files.pythonhosted.org/packages/24/bc/a4380f09bab3a776182578ce6b2771e57259d0d4dbce178205779abdc347/requests_html-0.10.0-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from requests_html) (2.23.0)\n",
            "Collecting w3lib\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/59/b6b14521090e7f42669cafdb84b0ab89301a42f1f1a82fcf5856661ea3a7/w3lib-1.22.0-py2.py3-none-any.whl\n",
            "Collecting pyppeteer>=0.0.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/05/ea3250282e46fda60df1f1d5246bb8cdc022abb89969c61a98ea28fd6b82/pyppeteer-0.2.5-py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.4MB/s \n",
            "\u001b[?25hCollecting fake-useragent\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
            "Collecting parse\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a1/82ce536be577ba09d4dcee45db58423a180873ad38a2d014d26ab7b7cb8a/parse-1.19.0.tar.gz\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from requests_html) (0.0.1)\n",
            "Collecting pyquery\n",
            "  Downloading https://files.pythonhosted.org/packages/58/0b/85d15e21f660a8ea68b1e0286168938857391f4ec9f6d204d91c9e013826/pyquery-1.4.3-py3-none-any.whl\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->requests_html) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->requests_html) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->requests_html) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->requests_html) (2.10)\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from w3lib->requests_html) (1.15.0)\n",
            "Collecting pyee<9.0.0,>=8.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0d/0a/933b3931107e1da186963fd9bb9bceb9a613cff034cb0fb3b0c61003f357/pyee-8.1.0-py2.py3-none-any.whl\n",
            "Collecting importlib-metadata<3.0.0,>=2.1.1; python_version < \"3.8\"\n",
            "  Downloading https://files.pythonhosted.org/packages/98/b8/8ec57a8ef46fbe7f185318c7ff7df9a06c9df451d9a59a067bfa851bb828/importlib_metadata-2.1.1-py2.py3-none-any.whl\n",
            "Collecting websockets<9.0,>=8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/d9/856af84843912e2853b1b6e898ac8b802989fcf9ecf8e8445a1da263bf3b/websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.6/dist-packages (from pyppeteer>=0.0.14->requests_html) (1.4.4)\n",
            "Collecting tqdm<5.0.0,>=4.42.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/02/8f8880a4fd6625461833abcf679d4c12a44c76f9925f92bf212bb6cefaad/tqdm-4.56.0-py2.py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->requests_html) (4.6.3)\n",
            "Collecting cssselect>0.7.9\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.6/dist-packages (from pyquery->requests_html) (4.2.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata<3.0.0,>=2.1.1; python_version < \"3.8\"->pyppeteer>=0.0.14->requests_html) (3.4.0)\n",
            "Building wheels for collected packages: fake-useragent, parse\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-cp36-none-any.whl size=13485 sha256=2bc3050bf486b0d2a7e9cc5a89066be865f3af6cd7c830ac8bb38a61d5df1024\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.19.0-cp36-none-any.whl size=24582 sha256=4cee6482ad84a4dc9e174cc14baa2c9c590a52a0dc874ef57dfc019dce3ae408\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/39/ea/e2fd678bd130953f5438470b8dfa529f00787e9b8b92b27467\n",
            "Successfully built fake-useragent parse\n",
            "\u001b[31mERROR: pyppeteer 0.2.5 has requirement urllib3<2.0.0,>=1.25.8, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: w3lib, pyee, importlib-metadata, websockets, tqdm, pyppeteer, fake-useragent, parse, cssselect, pyquery, requests-html\n",
            "  Found existing installation: importlib-metadata 3.4.0\n",
            "    Uninstalling importlib-metadata-3.4.0:\n",
            "      Successfully uninstalled importlib-metadata-3.4.0\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed cssselect-1.1.0 fake-useragent-0.1.11 importlib-metadata-2.1.1 parse-1.19.0 pyee-8.1.0 pyppeteer-0.2.5 pyquery-1.4.3 requests-html-0.10.0 tqdm-4.56.0 w3lib-1.22.0 websockets-8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTJD5Yb2pzK9",
        "outputId": "ec6b0c4b-97b3-498c-b845-2ce71709f936"
      },
      "source": [
        "# univariate lstm example\r\n",
        "import numpy as np\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import LSTM\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from yahoo_fin.stock_info import get_data\r\n",
        "import plotly.graph_objects as go\r\n",
        "\r\n",
        "\r\n",
        "# Global vars\r\n",
        "stock_list = [\"AKBNK.IS\"]\r\n",
        "for ticker in stock_list:\r\n",
        "\r\n",
        "    split_percent = 0.95\r\n",
        "\r\n",
        "\r\n",
        "    def get_price():\r\n",
        "        df_stock = get_data(ticker, start_date=None, end_date=None, index_as_date=False, interval=\"1d\")\r\n",
        "        df_stock = df_stock.dropna()\r\n",
        "        return df_stock\r\n",
        "\r\n",
        "    # preparing independent and dependent features\r\n",
        "    def prepare_data(timeseries_data, n_features):\r\n",
        "        X, y = [], []\r\n",
        "        for i in range(len(timeseries_data)):\r\n",
        "            # find the end of this pattern\r\n",
        "            end_ix = i + n_features\r\n",
        "            # check if we are beyond the sequence\r\n",
        "            if end_ix > len(timeseries_data) - 1:\r\n",
        "                break\r\n",
        "            # gather input and output parts of the pattern\r\n",
        "            seq_x, seq_y = timeseries_data[i:end_ix], timeseries_data[end_ix]\r\n",
        "            X.append(seq_x)\r\n",
        "            y.append(seq_y)\r\n",
        "        return np.array(X), np.array(y)\r\n",
        "\r\n",
        "\r\n",
        "    df = get_price()\r\n",
        "    close_data = df['close']\r\n",
        "    # define input sequence\r\n",
        "\r\n",
        "    n_steps = 60\r\n",
        "    split = int(split_percent * len(close_data))\r\n",
        "\r\n",
        "    close_train = close_data[:split]\r\n",
        "    close_test = close_data[split:]\r\n",
        "    close_train = np.array(close_train)\r\n",
        "    close_test = np.array(close_test)\r\n",
        "\r\n",
        "    # split into samples\r\n",
        "    X_train, y_train = prepare_data(close_train, n_steps)\r\n",
        "    X_test, y_test = prepare_data(close_test, n_steps)\r\n",
        "\r\n",
        "    print(X_train.shape)\r\n",
        "    print(X_test.shape)\r\n",
        "    print(y_train.shape)\r\n",
        "    print(y_test.shape)\r\n",
        "\r\n",
        "    # reshape from [samples, timesteps] into [samples, timesteps, features]\r\n",
        "    n_features = 1\r\n",
        "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\r\n",
        "    # define model\r\n",
        "    model = Sequential([\r\n",
        "            LSTM(120,input_shape=(X_train.shape[1],1),return_sequences=True),\r\n",
        "          #     Dropout(0.3),\r\n",
        "             LSTM(80,return_sequences=True),\r\n",
        "        #     Dropout(0.3),    \r\n",
        "            LSTM(60,return_sequences=False),\r\n",
        "        #     Dense(20),\r\n",
        "            Dense(1)\r\n",
        "        ])\r\n",
        "\r\n",
        "    model.compile(optimizer='rmsprop',loss='mse',metrics=['accuracy'])\r\n",
        "\r\n",
        "\r\n",
        "    # fit model\r\n",
        "    print(\"Model Train Start\")\r\n",
        "    model.fit(X_train, y_train, epochs=60, verbose=1)\r\n",
        "    print(\"Model Train Finish\")\r\n",
        "\r\n",
        "    print(\"Model Test Prediction Start\")\r\n",
        "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))\r\n",
        "    y_predict_test = model.predict(X_test, verbose=1)\r\n",
        "\r\n",
        "    y_predict_test = y_predict_test.reshape(-1)\r\n",
        "    print(\"Model Test Prediction Finish\")\r\n",
        "\r\n",
        "    # Predicting For the next 10 data\r\n",
        "\r\n",
        "    # demonstrate prediction for next 10 days\r\n",
        "    x_input = y_test[-60:]\r\n",
        "    temp_input = list(x_input)\r\n",
        "    lst_output = []\r\n",
        "    i = 0\r\n",
        "    while (i < 10):\r\n",
        "\r\n",
        "        if (len(temp_input) > 60):\r\n",
        "            x_input = np.array(temp_input[1:])\r\n",
        "            print(\"{} day input {}\".format(i, x_input))\r\n",
        "            # print(x_input)\r\n",
        "            x_input = x_input.reshape((1, n_steps, n_features))\r\n",
        "            # print(x_input)\r\n",
        "            yhat = model.predict(x_input, verbose=0)\r\n",
        "            print(\"{} day output {}\".format(i, yhat))\r\n",
        "            temp_input.append(yhat[0][0])\r\n",
        "            temp_input = temp_input[1:]\r\n",
        "            # print(temp_input)\r\n",
        "            lst_output.append(yhat[0][0])\r\n",
        "            i = i + 1\r\n",
        "        else:\r\n",
        "            x_input = x_input.reshape((1, n_steps, n_features))\r\n",
        "            yhat = model.predict(x_input, verbose=0)\r\n",
        "            print(yhat[0])\r\n",
        "            temp_input.append(yhat[0][0])\r\n",
        "            lst_output.append(yhat[0][0])\r\n",
        "            i = i + 1\r\n",
        "\r\n",
        "    print(lst_output)\r\n",
        "\r\n",
        "    # future_days = shift(lst_output, len(y_test), cval=np.NaN)\r\n",
        "\r\n",
        "    # show plt\r\n",
        "    trace1 = go.Scatter(\r\n",
        "        # x=train_data.index.tolist(),\r\n",
        "        y=y_test,\r\n",
        "        mode='lines',\r\n",
        "        name='Real Data'\r\n",
        "    )\r\n",
        "    trace2 = go.Scatter(\r\n",
        "        # x=test_data.index.tolist(),\r\n",
        "        y=y_predict_test,\r\n",
        "        mode='lines',\r\n",
        "        name='Predict Data'\r\n",
        "    )\r\n",
        "    trace3 = go.Scatter(\r\n",
        "        # x=test_data.index.tolist(),\r\n",
        "        y=lst_output,\r\n",
        "        mode='lines',\r\n",
        "        name='Future Data'\r\n",
        "    )\r\n",
        "\r\n",
        "    layout = go.Layout(\r\n",
        "        title=ticker + \"Stock\",\r\n",
        "        xaxis={'title': \"Date\"},\r\n",
        "        yaxis={'title': \"Close\"}\r\n",
        "    )\r\n",
        "\r\n",
        "    fig2 = go.Figure(data=[trace1, trace2, trace3], layout=layout)\r\n",
        "    fig2.write_html(ticker + 'p60n10.html', auto_open=False)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4889, 60)\n",
            "(201, 60)\n",
            "(4889,)\n",
            "(201,)\n",
            "Model Train Start\n",
            "Epoch 1/60\n",
            "153/153 [==============================] - 11s 12ms/step - loss: 2.7862 - accuracy: 0.0000e+00\n",
            "Epoch 2/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.2552 - accuracy: 0.0000e+00\n",
            "Epoch 3/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.1452 - accuracy: 0.0000e+00\n",
            "Epoch 4/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.1162 - accuracy: 0.0000e+00\n",
            "Epoch 5/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0813 - accuracy: 0.0000e+00\n",
            "Epoch 6/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0612 - accuracy: 0.0000e+00\n",
            "Epoch 7/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0619 - accuracy: 0.0000e+00\n",
            "Epoch 8/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0541 - accuracy: 0.0000e+00\n",
            "Epoch 9/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0489 - accuracy: 0.0000e+00\n",
            "Epoch 10/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0492 - accuracy: 0.0000e+00\n",
            "Epoch 11/60\n",
            "153/153 [==============================] - 2s 13ms/step - loss: 0.0422 - accuracy: 0.0000e+00\n",
            "Epoch 12/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0421 - accuracy: 0.0000e+00\n",
            "Epoch 13/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0379 - accuracy: 0.0000e+00\n",
            "Epoch 14/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0411 - accuracy: 0.0000e+00\n",
            "Epoch 15/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0384 - accuracy: 0.0000e+00\n",
            "Epoch 16/60\n",
            "153/153 [==============================] - 2s 13ms/step - loss: 0.0356 - accuracy: 0.0000e+00\n",
            "Epoch 17/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0355 - accuracy: 0.0000e+00\n",
            "Epoch 18/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0348 - accuracy: 0.0000e+00\n",
            "Epoch 19/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0304 - accuracy: 0.0000e+00\n",
            "Epoch 20/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0299 - accuracy: 0.0000e+00\n",
            "Epoch 21/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0327 - accuracy: 0.0000e+00\n",
            "Epoch 22/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0296 - accuracy: 0.0000e+00\n",
            "Epoch 23/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0310 - accuracy: 0.0000e+00\n",
            "Epoch 24/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0325 - accuracy: 0.0000e+00\n",
            "Epoch 25/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0284 - accuracy: 0.0000e+00\n",
            "Epoch 26/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0293 - accuracy: 0.0000e+00\n",
            "Epoch 27/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0278 - accuracy: 0.0000e+00\n",
            "Epoch 28/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0273 - accuracy: 0.0000e+00\n",
            "Epoch 29/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0270 - accuracy: 0.0000e+00\n",
            "Epoch 30/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0258 - accuracy: 0.0000e+00\n",
            "Epoch 31/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0287 - accuracy: 0.0000e+00\n",
            "Epoch 32/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0253 - accuracy: 0.0000e+00\n",
            "Epoch 33/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0280 - accuracy: 0.0000e+00\n",
            "Epoch 34/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0264 - accuracy: 0.0000e+00\n",
            "Epoch 35/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0276 - accuracy: 0.0000e+00\n",
            "Epoch 36/60\n",
            "153/153 [==============================] - 2s 13ms/step - loss: 0.0262 - accuracy: 0.0000e+00\n",
            "Epoch 37/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0259 - accuracy: 0.0000e+00\n",
            "Epoch 38/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0250 - accuracy: 0.0000e+00\n",
            "Epoch 39/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0253 - accuracy: 0.0000e+00\n",
            "Epoch 40/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0253 - accuracy: 0.0000e+00\n",
            "Epoch 41/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0245 - accuracy: 0.0000e+00\n",
            "Epoch 42/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0246 - accuracy: 0.0000e+00\n",
            "Epoch 43/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0257 - accuracy: 0.0000e+00\n",
            "Epoch 44/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0246 - accuracy: 0.0000e+00\n",
            "Epoch 45/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0241 - accuracy: 0.0000e+00\n",
            "Epoch 46/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0252 - accuracy: 0.0000e+00\n",
            "Epoch 47/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0251 - accuracy: 0.0000e+00\n",
            "Epoch 48/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0241 - accuracy: 0.0000e+00\n",
            "Epoch 49/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0241 - accuracy: 0.0000e+00\n",
            "Epoch 50/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0235 - accuracy: 0.0000e+00\n",
            "Epoch 51/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0223 - accuracy: 0.0000e+00\n",
            "Epoch 52/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0215 - accuracy: 0.0000e+00\n",
            "Epoch 53/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0242 - accuracy: 0.0000e+00\n",
            "Epoch 54/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0230 - accuracy: 0.0000e+00\n",
            "Epoch 55/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0206 - accuracy: 0.0000e+00\n",
            "Epoch 56/60\n",
            "153/153 [==============================] - 2s 11ms/step - loss: 0.0220 - accuracy: 0.0000e+00\n",
            "Epoch 57/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0220 - accuracy: 0.0000e+00\n",
            "Epoch 58/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0205 - accuracy: 0.0000e+00\n",
            "Epoch 59/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0218 - accuracy: 0.0000e+00\n",
            "Epoch 60/60\n",
            "153/153 [==============================] - 2s 12ms/step - loss: 0.0209 - accuracy: 0.0000e+00\n",
            "Model Train Finish\n",
            "Model Test Prediction Start\n",
            "7/7 [==============================] - 1s 5ms/step\n",
            "Model Test Prediction Finish\n",
            "[6.3645887]\n",
            "1 day input [5.38000011 5.6500001  6.21999979 6.26000023 6.30999994 6.32000017\n",
            " 6.         6.26999998 6.48999977 6.53000021 6.25       6.28999996\n",
            " 6.30000019 6.44000006 6.36999989 6.07999992 6.30999994 6.23999977\n",
            " 6.23999977 6.17999983 6.05000019 6.05999994 6.21999979 6.26999998\n",
            " 6.30000019 6.36999989 6.36999989 6.3499999  6.4000001  6.46999979\n",
            " 6.32000017 6.48999977 6.46999979 6.55999994 6.51999998 6.55000019\n",
            " 6.61999989 6.90999985 7.01000023 7.0999999  7.01000023 7.3499999\n",
            " 7.53000021 7.51999998 7.30999994 7.28000021 7.25       7.11999989\n",
            " 6.92000008 6.98999977 7.01999998 6.92000008 6.92000008 6.71999979\n",
            " 6.55999994 6.61000013 6.34000015 6.51000023 6.38000011 6.36458874]\n",
            "1 day output [[6.347856]]\n",
            "2 day input [5.6500001  6.21999979 6.26000023 6.30999994 6.32000017 6.\n",
            " 6.26999998 6.48999977 6.53000021 6.25       6.28999996 6.30000019\n",
            " 6.44000006 6.36999989 6.07999992 6.30999994 6.23999977 6.23999977\n",
            " 6.17999983 6.05000019 6.05999994 6.21999979 6.26999998 6.30000019\n",
            " 6.36999989 6.36999989 6.3499999  6.4000001  6.46999979 6.32000017\n",
            " 6.48999977 6.46999979 6.55999994 6.51999998 6.55000019 6.61999989\n",
            " 6.90999985 7.01000023 7.0999999  7.01000023 7.3499999  7.53000021\n",
            " 7.51999998 7.30999994 7.28000021 7.25       7.11999989 6.92000008\n",
            " 6.98999977 7.01999998 6.92000008 6.92000008 6.71999979 6.55999994\n",
            " 6.61000013 6.34000015 6.51000023 6.38000011 6.36458874 6.34785604]\n",
            "2 day output [[6.3263965]]\n",
            "3 day input [6.21999979 6.26000023 6.30999994 6.32000017 6.         6.26999998\n",
            " 6.48999977 6.53000021 6.25       6.28999996 6.30000019 6.44000006\n",
            " 6.36999989 6.07999992 6.30999994 6.23999977 6.23999977 6.17999983\n",
            " 6.05000019 6.05999994 6.21999979 6.26999998 6.30000019 6.36999989\n",
            " 6.36999989 6.3499999  6.4000001  6.46999979 6.32000017 6.48999977\n",
            " 6.46999979 6.55999994 6.51999998 6.55000019 6.61999989 6.90999985\n",
            " 7.01000023 7.0999999  7.01000023 7.3499999  7.53000021 7.51999998\n",
            " 7.30999994 7.28000021 7.25       7.11999989 6.92000008 6.98999977\n",
            " 7.01999998 6.92000008 6.92000008 6.71999979 6.55999994 6.61000013\n",
            " 6.34000015 6.51000023 6.38000011 6.36458874 6.34785604 6.32639647]\n",
            "3 day output [[6.3025956]]\n",
            "4 day input [6.26000023 6.30999994 6.32000017 6.         6.26999998 6.48999977\n",
            " 6.53000021 6.25       6.28999996 6.30000019 6.44000006 6.36999989\n",
            " 6.07999992 6.30999994 6.23999977 6.23999977 6.17999983 6.05000019\n",
            " 6.05999994 6.21999979 6.26999998 6.30000019 6.36999989 6.36999989\n",
            " 6.3499999  6.4000001  6.46999979 6.32000017 6.48999977 6.46999979\n",
            " 6.55999994 6.51999998 6.55000019 6.61999989 6.90999985 7.01000023\n",
            " 7.0999999  7.01000023 7.3499999  7.53000021 7.51999998 7.30999994\n",
            " 7.28000021 7.25       7.11999989 6.92000008 6.98999977 7.01999998\n",
            " 6.92000008 6.92000008 6.71999979 6.55999994 6.61000013 6.34000015\n",
            " 6.51000023 6.38000011 6.36458874 6.34785604 6.32639647 6.30259562]\n",
            "4 day output [[6.2742233]]\n",
            "5 day input [6.30999994 6.32000017 6.         6.26999998 6.48999977 6.53000021\n",
            " 6.25       6.28999996 6.30000019 6.44000006 6.36999989 6.07999992\n",
            " 6.30999994 6.23999977 6.23999977 6.17999983 6.05000019 6.05999994\n",
            " 6.21999979 6.26999998 6.30000019 6.36999989 6.36999989 6.3499999\n",
            " 6.4000001  6.46999979 6.32000017 6.48999977 6.46999979 6.55999994\n",
            " 6.51999998 6.55000019 6.61999989 6.90999985 7.01000023 7.0999999\n",
            " 7.01000023 7.3499999  7.53000021 7.51999998 7.30999994 7.28000021\n",
            " 7.25       7.11999989 6.92000008 6.98999977 7.01999998 6.92000008\n",
            " 6.92000008 6.71999979 6.55999994 6.61000013 6.34000015 6.51000023\n",
            " 6.38000011 6.36458874 6.34785604 6.32639647 6.30259562 6.27422333]\n",
            "5 day output [[6.242359]]\n",
            "6 day input [6.32000017 6.         6.26999998 6.48999977 6.53000021 6.25\n",
            " 6.28999996 6.30000019 6.44000006 6.36999989 6.07999992 6.30999994\n",
            " 6.23999977 6.23999977 6.17999983 6.05000019 6.05999994 6.21999979\n",
            " 6.26999998 6.30000019 6.36999989 6.36999989 6.3499999  6.4000001\n",
            " 6.46999979 6.32000017 6.48999977 6.46999979 6.55999994 6.51999998\n",
            " 6.55000019 6.61999989 6.90999985 7.01000023 7.0999999  7.01000023\n",
            " 7.3499999  7.53000021 7.51999998 7.30999994 7.28000021 7.25\n",
            " 7.11999989 6.92000008 6.98999977 7.01999998 6.92000008 6.92000008\n",
            " 6.71999979 6.55999994 6.61000013 6.34000015 6.51000023 6.38000011\n",
            " 6.36458874 6.34785604 6.32639647 6.30259562 6.27422333 6.24235916]\n",
            "6 day output [[6.2069755]]\n",
            "7 day input [6.         6.26999998 6.48999977 6.53000021 6.25       6.28999996\n",
            " 6.30000019 6.44000006 6.36999989 6.07999992 6.30999994 6.23999977\n",
            " 6.23999977 6.17999983 6.05000019 6.05999994 6.21999979 6.26999998\n",
            " 6.30000019 6.36999989 6.36999989 6.3499999  6.4000001  6.46999979\n",
            " 6.32000017 6.48999977 6.46999979 6.55999994 6.51999998 6.55000019\n",
            " 6.61999989 6.90999985 7.01000023 7.0999999  7.01000023 7.3499999\n",
            " 7.53000021 7.51999998 7.30999994 7.28000021 7.25       7.11999989\n",
            " 6.92000008 6.98999977 7.01999998 6.92000008 6.92000008 6.71999979\n",
            " 6.55999994 6.61000013 6.34000015 6.51000023 6.38000011 6.36458874\n",
            " 6.34785604 6.32639647 6.30259562 6.27422333 6.24235916 6.20697546]\n",
            "7 day output [[6.168888]]\n",
            "8 day input [6.26999998 6.48999977 6.53000021 6.25       6.28999996 6.30000019\n",
            " 6.44000006 6.36999989 6.07999992 6.30999994 6.23999977 6.23999977\n",
            " 6.17999983 6.05000019 6.05999994 6.21999979 6.26999998 6.30000019\n",
            " 6.36999989 6.36999989 6.3499999  6.4000001  6.46999979 6.32000017\n",
            " 6.48999977 6.46999979 6.55999994 6.51999998 6.55000019 6.61999989\n",
            " 6.90999985 7.01000023 7.0999999  7.01000023 7.3499999  7.53000021\n",
            " 7.51999998 7.30999994 7.28000021 7.25       7.11999989 6.92000008\n",
            " 6.98999977 7.01999998 6.92000008 6.92000008 6.71999979 6.55999994\n",
            " 6.61000013 6.34000015 6.51000023 6.38000011 6.36458874 6.34785604\n",
            " 6.32639647 6.30259562 6.27422333 6.24235916 6.20697546 6.16888809]\n",
            "8 day output [[6.129148]]\n",
            "9 day input [6.48999977 6.53000021 6.25       6.28999996 6.30000019 6.44000006\n",
            " 6.36999989 6.07999992 6.30999994 6.23999977 6.23999977 6.17999983\n",
            " 6.05000019 6.05999994 6.21999979 6.26999998 6.30000019 6.36999989\n",
            " 6.36999989 6.3499999  6.4000001  6.46999979 6.32000017 6.48999977\n",
            " 6.46999979 6.55999994 6.51999998 6.55000019 6.61999989 6.90999985\n",
            " 7.01000023 7.0999999  7.01000023 7.3499999  7.53000021 7.51999998\n",
            " 7.30999994 7.28000021 7.25       7.11999989 6.92000008 6.98999977\n",
            " 7.01999998 6.92000008 6.92000008 6.71999979 6.55999994 6.61000013\n",
            " 6.34000015 6.51000023 6.38000011 6.36458874 6.34785604 6.32639647\n",
            " 6.30259562 6.27422333 6.24235916 6.20697546 6.16888809 6.12914801]\n",
            "9 day output [[6.088831]]\n",
            "[6.3645887, 6.347856, 6.3263965, 6.3025956, 6.2742233, 6.242359, 6.2069755, 6.168888, 6.129148, 6.088831]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}